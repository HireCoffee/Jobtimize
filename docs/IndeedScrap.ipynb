{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndeedScrap doc\n",
    "These functions allow to perform web scrapping on Indeed platform, to collect job detail.\n",
    "____\n",
    "Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Jobtimize.rotateproxies import RotateProxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get, Timeout\n",
    "from requests.exceptions import HTTPError, ProxyError\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import islice\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *scrapPage()* function allows to scrap an html document from the URL.  \n",
    "This is to avoid redundant code in the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapPage(url, proxy=None):\n",
    "    with get(url, proxies=proxy) as response:\n",
    "        page = BeautifulSoup(response.text, 'html.parser')\n",
    "    return page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *scrapID()* function collects the IDs of the job ads published on the active page.  \n",
    "This information is the argument for the *'data-jk'* attribute in *'jobsearch-SerpJobCard'* class divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapID(page):\n",
    "    resultCol = page.find(id=\"resultsCol\")\n",
    "    setID = {\n",
    "        jobcard[\"data-jk\"]\n",
    "        for jobcard in resultCol.findAll(\"div\",\n",
    "                                         {\"class\": \"jobsearch-SerpJobCard\"})\n",
    "    }\n",
    "    return setID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *stripmatch()* function gets the match and the number of pages visited for the current search.  \n",
    "Since the form of a number in thousands differs from one country to another, regular expressions are used to harmonize the result: a list greater than two means a result greater than one thousand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripmatch(page):\n",
    "    try:\n",
    "        text = page.find(id=\"searchCountPages\").text.strip()\n",
    "    except AttributeError:\n",
    "        repage = match = None\n",
    "    else:\n",
    "        numlist = [num for num in re.findall(r'-?\\d+\\.?\\d*', text)]\n",
    "        repage = int(numlist[0])\n",
    "        if len(numlist) == 2:\n",
    "            match = int(numlist[1])\n",
    "        else:\n",
    "            match = int(''.join(numlist[1:]))\n",
    "    return repage, match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`scrapIndeedID()`** function extracts the IDs for each job for each country searched for.  \n",
    "The site is divided into different country-independent subdomains (the site in one country does not have access to the data in the other), scraping is performed for each subdomain of the site.  \n",
    "The number of results per page is arbitrarily set to 50.  \n",
    "After page 101 of results, Indeed considers the ads to be irrelevant. These will not be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapIndeedID(searchList, countryList, prox=False):\n",
    "    setID = set()\n",
    "    for search in searchList:\n",
    "        search = search.replace(\" \", \"+\")\n",
    "        if prox: proxies = RotateProxies()\n",
    "        proxy = None\n",
    "        for country_general in countryList:\n",
    "            country = country_general.lower()\n",
    "            if country == \"us\": country = \"www\"  #\"us\" note redirected\n",
    "            listID = set()\n",
    "            limit = 50\n",
    "            start = repage = count = 0\n",
    "            match = None\n",
    "            while (repage <= 101 or len(listID) < match):\n",
    "                url = \"https://{}.indeed.com/jobs?q={}&limit={}&start={}\".format(\n",
    "                    country, search, limit, start)\n",
    "                if count % 50 == 0 and prox: proxy = proxies.next()\n",
    "                try:\n",
    "                    page = scrapPage(url, proxy)\n",
    "                except (Timeout, ProxyError):\n",
    "                    if prox:\n",
    "                        proxy = proxies.next()\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                except HTTPError:\n",
    "                    break\n",
    "                else:\n",
    "                    repage, match = stripmatch(page)\n",
    "                    count += 1\n",
    "                    if (match is None or repage < count):\n",
    "                        break\n",
    "                    else:\n",
    "\n",
    "                        listID = listID.union({(country_general, jobID)\n",
    "                                               for jobID in list(scrapID(page))\n",
    "                                               })\n",
    "                        start += limit\n",
    "            setID = setID.union(listID)\n",
    "    return setID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dicoFromScrap()* function extracts the desired data from a tuple of the country and the *jobID*. A scraping is then performed for each page of a job. The collected information is:\n",
    "\n",
    "- description: long description\n",
    "- country: 2-letter abbreviation of the country in the tuple\n",
    "- city: full city name\n",
    "- posted: job post creation date, precise date for job published in the last 30 days\n",
    "- header: post title\n",
    "- company: company name\n",
    "- type\\*: type of employment contract (employee, intern...)\n",
    "- category\\*: job category\n",
    "- url: post url redirection\n",
    "\n",
    "\n",
    "\\* As the information is not formatted by most sub-domains, it will be extracted using word processing algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicoFromScrap(args):\n",
    "    tupleID, proxy = args\n",
    "    dico = {}\n",
    "    url = \"https://www.indeed.com/viewjob?jk={}\".format(tupleID[1])\n",
    "    try:\n",
    "        page = scrapPage(url, proxy)\n",
    "    except HTTPError:\n",
    "        return dico\n",
    "\n",
    "    def postedDate(page):\n",
    "        try:\n",
    "            date = int(\n",
    "                re.findall(\n",
    "                    r'-?\\d+\\.?\\d*',\n",
    "                    page.find(\"div\", {\n",
    "                        \"class\": \"jobsearch-JobMetadataFooter\"\n",
    "                    }).text)[0])\n",
    "        except IndexError:\n",
    "            posted = datetime.now().isoformat(timespec='seconds')\n",
    "        else:\n",
    "            posted = (datetime.now() +\n",
    "                      timedelta(days=-date)).isoformat(timespec='seconds')\n",
    "            if date == 30: posted = \"+ \" + posted\n",
    "        return posted\n",
    "\n",
    "    def companyName(page):\n",
    "        try:\n",
    "            name = page.find(\"div\", {\"class\": \"icl-u-lg-mr--sm\"}).text\n",
    "        except AttributeError:\n",
    "            name = page.find(\"span\", {\n",
    "                \"class\": \"icl-u-textColor--success\"\n",
    "            }).text\n",
    "        except:\n",
    "            name = \"\"\n",
    "        return name\n",
    "\n",
    "    dico[\"country\"] = tupleID[0].upper()\n",
    "    dico[\"url\"] = url\n",
    "    dico[\"description\"] = page.find(id=\"jobDescriptionText\").text\n",
    "    dico[\"header\"], dico[\"city\"], *_ = page.head.title.text.split(\" - \")\n",
    "    dico[\"company\"] = companyName(page)\n",
    "    dico[\"type\"] = dico[\"category\"] = \"\"\n",
    "    dico[\"posted\"] = postedDate(page)\n",
    "\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`IndeedScrap()`** is the main function which collects and standardizes data on the Indeed site.  \n",
    "Threads are used depending on the size of the results for data normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IndeedScrap(searchList, countryList, prox=False):\n",
    "    scraped = list()\n",
    "    setID = scrapIndeedID(searchList, countryList, prox)\n",
    "\n",
    "    if len(setID) < 20:\n",
    "        workers = len(setID)\n",
    "    else:\n",
    "        workers = len(setID) / 5\n",
    "\n",
    "    if prox:\n",
    "        proxies = list(islice(RotateProxies().proxies, workers)) * len(setID)\n",
    "    else:\n",
    "        proxies = [None] * len(setID)\n",
    "\n",
    "    with ThreadPoolExecutor(workers) as executor:\n",
    "        try:\n",
    "            for result in executor.map(dicoFromScrap, zip(setID, proxies)):\n",
    "                scraped.append(result)\n",
    "        except:\n",
    "            pass\n",
    "    return scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of use\n",
    "Let's do research on the data analyst post in France in the city of Rennes.  \n",
    "Preview the 2nd item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'FR',\n",
       " 'url': 'https://www.indeed.com/viewjob?jk=5ee4ff46ba9d8f78',\n",
       " 'description': \"Flux-Vision est un produit pensé, conçu, développé et opéré par une seule et même équipe. Ceci confère à l'équipe une grande autonomie sur l'organisation (agile) et les choix techniques.\\nDans le cadre de l'industrialisation et de l'optimisation des performances des briques logicielles du service Flux Vision, nous cherchons un développeur/concepteur/architecte logiciel pour venir épauler l'équipe de développement de Rennes.\\nLes composants logiciels à gérer vont du portail client (java full stack) aux outils de gestion des logiciels de calculs (python), en passant par les briques d'intégrations et à l'automatisation des déploiements et de l'exploitation (power-shell, ansible, splunk, grafana…).\\nNos environnements sont Windows Serveur, Amazon AWS et Linux.\\nDescription de la mission:\\nAu sein de l'équipe de développement, selon votre profil, vous interviendrez en tant qu'architecte logiciel, /développeur sur les technologies java full stack (AngularJS et Angular8)\\nVous intervenez sur tout le cycle de vie d'un/de plusieurs composants logiciels :\\nDéfinition du besoin (en collaboration avec les autres membres de l'équipe de développement et des équipes de data-analystes Flux-Vision)\\nDéfinition de la solution logicielle\\nDéveloppement et tests\\nSupport et maintenance des versions en production\\nSi vous avez une compétence AWS, des travaux sont aussi à effectuer sur cette plateforme.\\nabout you\\nVous êtes titulaire d'un Bac+5 master ou école d'ingénieur dans le domaine de l'IT. Vous avez une expérience significative dans la mise en place de logiciels en java fullstack Angular. Vous souhaitez vous impliquer sur tout le cycle de vie d'un produit logiciel. Une compétence AWS serait un plus.\\nDynamique, doté(e) d'un bon relationnel, vous aimez travailler en équipe. Vous souhaitez intégrer une structure qui saura être à l'écoute de votre potentiel et qui vous permettra d'évoluer, alors envoyez sans plus attendre votre candidature.\\ndepartment\\nDigital & Data\\nPartenaire de la transformation digitale des entreprises, Orange Digital&Data est l'entité d'Orange Business Services spécialisée dans la conception et le développement de services applicatifs et l'intégration de systèmes.\\n\\nImplantés dans plusieurs grandes villes françaises comme Paris, Rennes, Lyon, Bordeaux, Lille et Toulouse … nous accompagnons au quotidien près de 20 000 entreprises tout au long du cycle de vie de leurs projets, dans les domaines clés de l'expérience digitale, de la Data Analytics et l’Intelligence Métier.\\n\\nPour la 4ème année consécutive, Orange reçoit la certification « Top Employer Global » 2019. Cette certification consacre les meilleures politiques et pratiques en termes de programmes de ressources humaines.\\nL’innovation est essentielle à votre métier, construisons la ensemble !\\ncontract\\nCDI\",\n",
       " 'header': \"Développeur / Architecte logiciel au sein de l'équipe Flux-vision F/H\",\n",
       " 'city': 'Rennes (35)',\n",
       " 'company': 'Orange',\n",
       " 'type': '',\n",
       " 'category': '',\n",
       " 'posted': '2020-01-30T02:38:39'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfJob = IndeedScrap([\"Data Analyst rennes\"],[\"FR\"])\n",
    "listOfJob[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
