{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndeedScrap doc\n",
    "These functions allow to perform web scrapping on Indeed platform, to collect job detail.\n",
    "____\n",
    "Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, HTTPError\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *scrapPage()* function allows to scrap an html document from the URL.  \n",
    "This is to avoid redundant code in the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapPage(url):\n",
    "    with urlopen(url) as response:\n",
    "        page = BeautifulSoup(response.read(), 'html.parser')\n",
    "    return page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *scrapID()* function collects the IDs of the job ads published on the active page.  \n",
    "This information is the argument for the *'data-jk'* attribute in *'jobsearch-SerpJobCard'* class divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapID(page):\n",
    "    resultCol = page.find(id=\"resultsCol\")\n",
    "    setID = {\n",
    "        jobcard[\"data-jk\"]\n",
    "        for jobcard in resultCol.findAll(\"div\",\n",
    "                                         {\"class\": \"jobsearch-SerpJobCard\"})\n",
    "    }\n",
    "    return setID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *stripmatch()* function gets the match and the number of pages visited for the current search.  \n",
    "Since the form of a number in thousands differs from one country to another, regular expressions are used to harmonize the result: a list greater than two means a result greater than one thousand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripmatch(page):\n",
    "    try:\n",
    "        text = page.find(id=\"searchCountPages\").text.strip()\n",
    "    except AttributeError:\n",
    "        repage = match = None\n",
    "    else:\n",
    "        numlist = [num for num in re.findall(r'-?\\d+\\.?\\d*', text)]\n",
    "        repage = int(numlist[0])\n",
    "        if len(numlist) == 2:\n",
    "            match = int(numlist[1])\n",
    "        else:\n",
    "            match = int(''.join(numlist[1:]))\n",
    "    return repage, match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`scrapIndeedID()`** function extracts the IDs for each job for each country searched for.  \n",
    "The site is divided into different country-independent subdomains (the site in one country does not have access to the data in the other), scraping is performed for each subdomain of the site.  \n",
    "The number of results per page is arbitrarily set to 50.  \n",
    "After page 101 of results, Indeed considers the ads to be irrelevant. These will not be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapIndeedID(searchList, countryList):\n",
    "    setID = set()\n",
    "    for search in searchList:\n",
    "        search = search.replace(\" \", \"+\")\n",
    "        for country in countryList:\n",
    "            country = country.lower()\n",
    "            listID = set()\n",
    "            limit = 50\n",
    "            start = repage = count = 0\n",
    "            match = None\n",
    "            while (repage <= 101 or len(listID) < match):\n",
    "                url = \"https://{}.indeed.com/jobs?q={}&limit={}&start={}\".format(\n",
    "                    country, search, limit, start)\n",
    "                try:\n",
    "                    page = scrapPage(url)\n",
    "                except HTTPError:\n",
    "                    break\n",
    "                else:\n",
    "                    repage, match = stripmatch(page)\n",
    "                    count += 1\n",
    "                    if (match is None or repage < count):\n",
    "                        break\n",
    "                    else:\n",
    "                        listID = listID.union({(country, jobID)\n",
    "                                               for jobID in list(scrapID(page))\n",
    "                                               })\n",
    "                        start += limit\n",
    "            setID = setID.union(listID)\n",
    "    return setID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dicoFromScrap()* function extracts the desired data from a tuple of the country and the *jobID*. A scraping is then performed for each page of a job. The collected information is:\n",
    "\n",
    "- description: long description\n",
    "- country: 2-letter abbreviation of the country in the tuple\n",
    "- city: full city name\n",
    "- posted: job post creation date, precise date for job published in the last 30 days\n",
    "- header: post title\n",
    "- company: company name\n",
    "- type\\*: type of employment contract (employee, intern...)\n",
    "- category\\*: job category\n",
    "- url: post url redirection\n",
    "\n",
    "\n",
    "\\* As the information is not formatted by most sub-domains, it will be extracted using word processing algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicoFromScrap(tupleID):\n",
    "    dico = {}\n",
    "    url = \"https://www.indeed.com/viewjob?jk={}\".format(tupleID[1])\n",
    "    try:\n",
    "        page = scrapPage(url)\n",
    "    except HTTPError:\n",
    "        return dico\n",
    "\n",
    "    def postedDate(page):\n",
    "        try:\n",
    "            date = int(\n",
    "                re.findall(\n",
    "                    r'-?\\d+\\.?\\d*',\n",
    "                    page.find(\"div\", {\n",
    "                        \"class\": \"jobsearch-JobMetadataFooter\"\n",
    "                    }).text)[0])\n",
    "        except IndexError:\n",
    "            posted = datetime.now().isoformat(timespec='seconds')\n",
    "        else:\n",
    "            posted = (datetime.now() +\n",
    "                      timedelta(days=-date)).isoformat(timespec='seconds')\n",
    "            if date == 30: posted = \"+ \" + posted\n",
    "        return posted\n",
    "\n",
    "    def companyName(page):\n",
    "        try:\n",
    "            name = page.find(\"div\", {\"class\": \"icl-u-lg-mr--sm\"}).text\n",
    "        except AttributeError:\n",
    "            name = page.find(\"span\", {\n",
    "                \"class\": \"icl-u-textColor--success\"\n",
    "            }).text\n",
    "        except:\n",
    "            name = \"\"\n",
    "        return name\n",
    "\n",
    "    dico[\"country\"] = tupleID[0].upper()\n",
    "    dico[\"url\"] = url\n",
    "    dico[\"description\"] = page.find(id=\"jobDescriptionText\").text\n",
    "    dico[\"header\"], dico[\"city\"], *_ = page.head.title.text.split(\" - \")\n",
    "    dico[\"company\"] = companyName(page)\n",
    "    dico[\"type\"] = dico[\"category\"] = \"\"\n",
    "    dico[\"posted\"] = postedDate(page)\n",
    "\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`IndeedScrap()`** is the main function which collects and standardizes data on the Indeed site.  \n",
    "Threads are used depending on the size of the results for data normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IndeedScrap(searchList, countryList):\n",
    "    scraped = list()\n",
    "    setID = scrapIndeedID(searchList, countryList)\n",
    "    if len(setID) < 20:\n",
    "        workers = len(setID)\n",
    "    else:\n",
    "        workers = len(setID) / 5\n",
    "    with ThreadPoolExecutor(workers) as executor:\n",
    "        for result in executor.map(dicoFromScrap, setID):\n",
    "            scraped.append(result)\n",
    "    return scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of use\n",
    "Let's do research on the data analyst post in France in the city of Rennes.  \n",
    "Preview the 2nd item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'FR',\n",
       " 'url': 'https://www.indeed.com/viewjob?jk=b0b2a0eb1416388c',\n",
       " 'description': \"Nous sommes spécialisés dans la maintenance, la rénovation et la réparation industrielle de produits électroniques. Autour de ce cœur de métier, nous avons développé une gamme de services et proposons à nos clients une gestion globale d'activité : supply chain, centres d’appels, e-boutiques, sites web dédiés, prestations techniques en boutiques, reprise et recyclage de produits, ingénierie R&D.\\n\\nLa forte croissance enregistrée ces dernières années a permis à Cordon Electronics de se hisser parmi les 5 premières entreprises européennes de son secteur, en particulier dans le domaine des décodeurs et des terminaux numériques. En France, Cordon Electronics est considéré comme le leader dans la maintenance des terminaux de télécommunication\\n\\nAujourd’hui, 12 centres de réparation en France, 8 filiales hors métropole, des bureaux à Paris.\\nDescription du poste\\n\\nNous recrutons en stage, un(e) Data Analyst (H/F).\\n\\nVos missions :\\n\\nAu sein de l’équipe décisionnel du Groupe composée de 4 collaborateurs et rattaché(e) à la Responsable du service, vous aurez pour mission de :\\nComprendre les missions et enjeux du groupeAnalyser les donnéesEtudier la mise en place d’algorithme de Machine LearningTester les performances de différents modèles de Machine Learning.\\n\\nVotre objectif : E-exploiter les données numériques de notre système d’information afin de synthétiser et de restituer les informations pour leur donner du sens.\\n\\nL’accompagnement de votre tutrice et de l’équipe vous permettra de mener à bien vos missions.\\nRejoignez-nous pour vivre un vrai projet de Data Analysis !\\nVotre Profil\\n\\nVous êtes en cours de formation de niveau Bac +3 à Bac +5 dans le domaine Informatique, Décisionnel ou Statistique et vous bénéficiez d’une première expérience en stage sur une mission similaire.\\n\\nCompétences techniques mises en œuvre lors de la mission :\\nLangage SQLPython, RElastic Search, Kibana et LogstashData Mining / BI.\\n\\nAptitudes professionnelles :\\nInvesti(e), vous êtes force de proposition dans un projet d'entrepriseDynamique, vous avez l'esprit d'équipe et la culture du résultatEnthousiaste, vous souhaitez développer une réelle expertise métier web et technique.\\n\\nStage basé à Rennes/Cesson-Sévigné (35).\\n\\nMission à pourvoir pour une durée de 3 à 6 mois.\\nrecrutement@cordongroup.com avec CV & Lettre de motivation\\n\\nNos valeurs sont fondées autour de la proximité, de la réactivité, de l'innovation, de la loyauté et du travail en équipe.\\n\\nREJOIGNEZ-NOUS !\",\n",
       " 'header': 'Data Analyst (H/F)',\n",
       " 'city': '35510 Cesson-Sévigné',\n",
       " 'company': 'CORDON ELECTRONICS SAS',\n",
       " 'type': '',\n",
       " 'category': '',\n",
       " 'posted': '2020-01-03T23:13:37'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfJob = IndeedScrap([\"Data Analyst rennes\"],[\"FR\"])\n",
    "listOfJob[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
