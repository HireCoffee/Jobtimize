{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndeedScrap doc\n",
    "These functions allow to perform web scrapping on Indeed platform, to collect job detail.\n",
    "____\n",
    "Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Jobtimize.rotateproxies import RotateProxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get, Timeout\n",
    "from requests.exceptions import HTTPError, ProxyError\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import islice\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *scrapPage()* function allows to scrap an html document from the URL.  \n",
    "This is to avoid redundant code in the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapPage(url, proxy=None):\n",
    "    with get(url, proxies=proxy) as response:\n",
    "        page = BeautifulSoup(response.text, 'html.parser')\n",
    "    return page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *scrapID()* function collects the IDs of the job ads published on the active page.  \n",
    "This information is the argument for the *'data-jk'* attribute in *'jobsearch-SerpJobCard'* class divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapID(page):\n",
    "    resultCol = page.find(id=\"resultsCol\")\n",
    "    setID = {\n",
    "        jobcard[\"data-jk\"]\n",
    "        for jobcard in resultCol.findAll(\"div\",\n",
    "                                         {\"class\": \"jobsearch-SerpJobCard\"})\n",
    "    }\n",
    "    return setID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *stripmatch()* function gets the match and the number of pages visited for the current search.  \n",
    "Since the form of a number in thousands differs from one country to another, regular expressions are used to harmonize the result: a list greater than two means a result greater than one thousand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripmatch(page):\n",
    "    try:\n",
    "        text = page.find(id=\"searchCountPages\").text.strip()\n",
    "    except AttributeError:\n",
    "        repage = match = None\n",
    "    else:\n",
    "        numlist = [num for num in re.findall(r'-?\\d+\\.?\\d*', text)]\n",
    "        repage = int(numlist[0])\n",
    "        if len(numlist) == 2:\n",
    "            match = int(numlist[1])\n",
    "        else:\n",
    "            match = int(''.join(numlist[1:]))\n",
    "    return repage, match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`scrapIndeedID()`** function extracts the IDs for each job for each country searched for.  \n",
    "The site is divided into different country-independent subdomains (the site in one country does not have access to the data in the other), scraping is performed for each subdomain of the site.  \n",
    "The number of results per page is arbitrarily set to 50.  \n",
    "After page 101 of results, Indeed considers the ads to be irrelevant. These will not be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapIndeedID(searchList, countryList, prox=False):\n",
    "    setID = set()\n",
    "    for search in searchList:\n",
    "        search = search.replace(\" \", \"+\")\n",
    "        if prox: proxies = RotateProxies()\n",
    "        proxy = None\n",
    "        for country_general in countryList:\n",
    "            country = country_general.lower()\n",
    "            if country == \"us\": country = \"www\"  #\"us\" note redirected\n",
    "            listID = set()\n",
    "            limit = 50\n",
    "            start = repage = count = 0\n",
    "            match = None\n",
    "            while (repage <= 101 or len(listID) < match):\n",
    "                url = \"https://{}.indeed.com/jobs?q={}&limit={}&start={}\".format(\n",
    "                    country, search, limit, start)\n",
    "                if count % 50 == 0 and prox: proxy = proxies.next()\n",
    "                try:\n",
    "                    page = scrapPage(url, proxy)\n",
    "                except (Timeout, ProxyError):\n",
    "                    if prox:\n",
    "                        proxy = proxies.next()\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                except HTTPError:\n",
    "                    break\n",
    "                else:\n",
    "                    repage, match = stripmatch(page)\n",
    "                    count += 1\n",
    "                    if (match is None or repage < count):\n",
    "                        break\n",
    "                    else:\n",
    "\n",
    "                        listID = listID.union({(country_general, jobID)\n",
    "                                               for jobID in list(scrapID(page))\n",
    "                                               })\n",
    "                        start += limit\n",
    "            setID = setID.union(listID)\n",
    "    return setID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dicoFromScrap()* function extracts the desired data from a tuple of the country and the *jobID*. A scraping is then performed for each page of a job. The collected information is:\n",
    "\n",
    "- description: long description\n",
    "- country: 2-letter abbreviation of the country in the tuple\n",
    "- city: full city name\n",
    "- posted: job post creation date, precise date for job published in the last 30 days\n",
    "- header: post title\n",
    "- company: company name\n",
    "- type\\*: type of employment contract (employee, intern...)\n",
    "- category\\*: job category\n",
    "- url: post url redirection\n",
    "\n",
    "\n",
    "\\* As the information is not formatted by most sub-domains, it will be extracted using word processing algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicoFromScrap(args):\n",
    "    tupleID, proxy = args\n",
    "    dico = {}\n",
    "    url = \"https://www.indeed.com/viewjob?jk={}\".format(tupleID[1])\n",
    "    try:\n",
    "        page = scrapPage(url, proxy)\n",
    "    except HTTPError:\n",
    "        return dico\n",
    "\n",
    "    def postedDate(page):\n",
    "        try:\n",
    "            date = int(\n",
    "                re.findall(\n",
    "                    r'-?\\d+\\.?\\d*',\n",
    "                    page.find(\"div\", {\n",
    "                        \"class\": \"jobsearch-JobMetadataFooter\"\n",
    "                    }).text)[0])\n",
    "        except IndexError:\n",
    "            posted = datetime.now().isoformat(timespec='seconds')\n",
    "        else:\n",
    "            posted = (datetime.now() +\n",
    "                      timedelta(days=-date)).isoformat(timespec='seconds')\n",
    "            if date == 30: posted = \"+ \" + posted\n",
    "        return posted\n",
    "\n",
    "    def companyName(page):\n",
    "        try:\n",
    "            name = page.find(\"div\", {\"class\": \"icl-u-lg-mr--sm\"}).text\n",
    "        except AttributeError:\n",
    "            name = page.find(\"span\", {\n",
    "                \"class\": \"icl-u-textColor--success\"\n",
    "            }).text\n",
    "        except:\n",
    "            name = \"\"\n",
    "        return name\n",
    "\n",
    "    dico[\"country\"] = tupleID[0].upper()\n",
    "    dico[\"url\"] = url\n",
    "    dico[\"description\"] = page.find(id=\"jobDescriptionText\").text\n",
    "    dico[\"header\"], dico[\"city\"], *_ = page.head.title.text.split(\" - \")\n",
    "    dico[\"company\"] = companyName(page)\n",
    "    dico[\"type\"] = dico[\"category\"] = \"\"\n",
    "    dico[\"posted\"] = postedDate(page)\n",
    "\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`IndeedScrap()`** is the main function which collects and standardizes data on the Indeed site.  \n",
    "Threads are used depending on the size of the results for data normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IndeedScrap(searchList, countryList, prox=False):\n",
    "    scraped = list()\n",
    "    setID = scrapIndeedID(searchList, countryList, prox)\n",
    "\n",
    "    if len(setID) < 20:\n",
    "        workers = len(setID)\n",
    "    else:\n",
    "        workers = len(setID) / 5\n",
    "\n",
    "    if prox:\n",
    "        proxies = list(islice(RotateProxies().proxies, workers)) * len(setID)\n",
    "    else:\n",
    "        proxies = [None] * len(setID)\n",
    "\n",
    "    with ThreadPoolExecutor(workers) as executor:\n",
    "        try:\n",
    "            for result in executor.map(dicoFromScrap, zip(setID, proxies)):\n",
    "                scraped.append(result)\n",
    "        except:\n",
    "            pass\n",
    "    return scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of use\n",
    "Let's do research on the data analyst post in France in the city of Rennes.  \n",
    "Preview the 2nd item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'FR',\n",
       " 'url': 'https://www.indeed.com/viewjob?jk=c24f0ebf86217624',\n",
       " 'description': 'Notre société est spécialisée dans la formation à distance.Basée à Rennes, nous continuons de renforcer nos équipes et recherchons dans la cadre de notre croissance :1 DATA ANALYST H/F - CDI, - Temps plein.Vos responsabilités, vous prenez en charge:La gestion de notre BDD clients sur ERP et Plateforme E Learning.Le traitement des données sur l\\'ERP.La gestion complexe de tableaux à des fins de pubipostage.L\\'étude des flux de données clients à des fins de simplificationLa parfaite maitrise de notre ERP à des fins de support aux équipes.Votre profil : De formation supérieure,Vous avez une parfaite maîtrise de la suite PACK OFFICE et une maîtrise \"expert\" d\\'Excel.Vous êtes par nature rigoureux.Vous êtes dynamique.Vous appréciez le travail en équipe et êtes investi dans vos missions.Vous êtes force de proposition pour faire progresser l\\'entreprise.Présentation de l’entreprise : Nous avons un haut niveau d\\'exigence de la satisfaction client qui repose sur l’implication et l\\'adhésion de tous les salariés à cet objectif.Nous sommes certifiés AFNOR NF Service, 1ère entreprise en France dans notre secteur d\\'activité.L\\'entreprise est co-dirigée par un binôme femme/homme, parfaitement complémentaires.Notre positionnement tarifaire est atypique puisque nous sommes \"à prix très accessibles\" et offrons un rapport qualité/prix inégalé.Nous vous proposons : Un cadre de travail agréable.Une forte dynamique du fait de notre croissance.Une formation interne.6 semaines de congés.Rare : prévoyance pour les non cadres.1 cadre de travail à taille humaine.Amplitude horaire : Lundi-Vendredi (08h30 - 18H00).Merci de nous faire parvenir un CV et quelques lignes de votre motivation pour ce poste.Notre processus de recrutement inclut un premier échange téléphonique qui, s\\'il est concluant, aboutira à 2ème entretien puis à des tests.Nos sites internet : www.espace-concours.frwww.ascor-communication.frType d\\'emploi : Temps plein, CDISalaire : 22\\xa0000,00€ à 30\\xa0000,00€ /anExpérience:data analyst h/f ou similaire: 1 an (Souhaité)Télétravail:Oui',\n",
       " 'header': 'Data Analyst H/F',\n",
       " 'city': 'Rennes (35)',\n",
       " 'company': 'Ascor Communication',\n",
       " 'type': '',\n",
       " 'category': '',\n",
       " 'posted': '2020-05-18T11:45:12'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfJob = IndeedScrap([\"Data Analyst rennes\"],[\"FR\"])\n",
    "listOfJob[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
