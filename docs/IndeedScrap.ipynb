{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndeedScrap doc\n",
    "These functions allow to perform web scrapping on Indeed platform, to collect job detail.\n",
    "____\n",
    "Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, HTTPError\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *scrapPage()* function allows to scrap an html document from the URL.  \n",
    "This is to avoid redundant code in the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapPage(url):\n",
    "    with urlopen(url) as response:\n",
    "        page = BeautifulSoup(response.read(), 'html.parser')\n",
    "    return page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *scrapID()* function collects the IDs of the job ads published on the active page.  \n",
    "This information is the argument for the *'data-jk'* attribute in *'jobsearch-SerpJobCard'* class divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapID(page):\n",
    "    resultCol = page.find(id=\"resultsCol\")\n",
    "    setID = {\n",
    "        jobcard[\"data-jk\"]\n",
    "        for jobcard in resultCol.findAll(\"div\",\n",
    "                                         {\"class\": \"jobsearch-SerpJobCard\"})\n",
    "    }\n",
    "    return setID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *stripmatch()* function gets the match and the number of pages visited for the current search.  \n",
    "Since the form of a number in thousands differs from one country to another, regular expressions are used to harmonize the result: a list greater than two means a result greater than one thousand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripmatch(page):\n",
    "    try:\n",
    "        text = page.find(id=\"searchCountPages\").text.strip()\n",
    "    except AttributeError:\n",
    "        repage = match = None\n",
    "    else:\n",
    "        numlist = [num for num in re.findall(r'-?\\d+\\.?\\d*', text)]\n",
    "        repage = int(numlist[0])\n",
    "        if len(numlist) == 2:\n",
    "            match = int(numlist[1])\n",
    "        else:\n",
    "            match = int(''.join(numlist[1:]))\n",
    "    return repage, match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`scrapIndeedID()`** function extracts the IDs for each job for each country searched for.  \n",
    "The site is divided into different country-independent subdomains (the site in one country does not have access to the data in the other), scraping is performed for each subdomain of the site.  \n",
    "The number of results per page is arbitrarily set to 50.  \n",
    "After page 101 of results, Indeed considers the ads to be irrelevant. These will not be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapIndeedID(searchList, countryList):\n",
    "    setID = set()\n",
    "    for search in searchList:\n",
    "        search = search.replace(\" \", \"+\")\n",
    "        for country in countryList:\n",
    "            country = country.lower()\n",
    "            listID = set()\n",
    "            limit = 50\n",
    "            start = repage = count = 0\n",
    "            match = None\n",
    "            while (repage <= 101 or len(listID) < match):\n",
    "                url = \"https://{}.indeed.com/jobs?q={}&limit={}&start={}\".format(\n",
    "                    country, search, limit, start)\n",
    "                try:\n",
    "                    page = scrapPage(url)\n",
    "                except HTTPError:\n",
    "                    break\n",
    "                else:\n",
    "                    repage, match = stripmatch(page)\n",
    "                    count += 1\n",
    "                    if (match is None or repage < count):\n",
    "                        break\n",
    "                    else:\n",
    "                        listID = listID.union({(country, jobID)\n",
    "                                               for jobID in list(scrapID(page))\n",
    "                                               })\n",
    "                        start += limit\n",
    "            setID = setID.union(listID)\n",
    "    return setID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dicoFromScrap()* function extracts the desired data from a tuple of the country and the *jobID*. A scraping is then performed for each page of a job. The collected information is:\n",
    "\n",
    "- description: long description\n",
    "- country: 2-letter abbreviation of the country in the tuple\n",
    "- city: full city name\n",
    "- posted: job post creation date, precise date for job published in the last 30 days\n",
    "- header: post title\n",
    "- company: company name\n",
    "- type\\*: type of employment contract (employee, intern...)\n",
    "- category\\*: job category\n",
    "- url: post url redirection\n",
    "\n",
    "\n",
    "\\* As the information is not formatted by most sub-domains, it will be extracted using word processing algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicoFromScrap(tupleID):\n",
    "    dico = {}\n",
    "    url = \"https://www.indeed.com/viewjob?jk={}\".format(tupleID[1])\n",
    "    try:\n",
    "        page = scrapPage(url)\n",
    "    except HTTPError:\n",
    "        return dico\n",
    "\n",
    "    def postedDate(page):\n",
    "        try:\n",
    "            date = int(\n",
    "                re.findall(\n",
    "                    r'-?\\d+\\.?\\d*',\n",
    "                    page.find(\"div\", {\n",
    "                        \"class\": \"jobsearch-JobMetadataFooter\"\n",
    "                    }).text)[0])\n",
    "        except IndexError:\n",
    "            posted = datetime.now().isoformat(timespec='seconds')\n",
    "        else:\n",
    "            posted = (datetime.now() +\n",
    "                      timedelta(days=-date)).isoformat(timespec='seconds')\n",
    "            if date == 30: posted = \"+ \" + posted\n",
    "        return posted\n",
    "\n",
    "    def companyName(page):\n",
    "        try:\n",
    "            name = page.find(\"div\", {\"class\": \"icl-u-lg-mr--sm\"}).text\n",
    "        except AttributeError:\n",
    "            name = page.find(\"span\", {\n",
    "                \"class\": \"icl-u-textColor--success\"\n",
    "            }).text\n",
    "        except:\n",
    "            name = \"\"\n",
    "        return name\n",
    "\n",
    "    dico[\"country\"] = tupleID[0].upper()\n",
    "    dico[\"url\"] = url\n",
    "    dico[\"description\"] = page.find(id=\"jobDescriptionText\").text\n",
    "    dico[\"header\"], dico[\"city\"], *_ = page.head.title.text.split(\" - \")\n",
    "    dico[\"company\"] = companyName(page)\n",
    "    dico[\"type\"] = dico[\"category\"] = \"\"\n",
    "    dico[\"posted\"] = postedDate(page)\n",
    "\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`IndeedScrap()`** is the main function which collects and standardizes data on the Indeed site.  \n",
    "Threads are used depending on the size of the results for data normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IndeedScrap(searchList, countryList):\n",
    "    scraped = list()\n",
    "    setID = scrapIndeedID(searchList, countryList)\n",
    "    if len(setID) < 20:\n",
    "        workers = len(setID)\n",
    "    else:\n",
    "        workers = len(setID) / 5\n",
    "    with ThreadPoolExecutor(workers) as executor:\n",
    "        for result in executor.map(dicoFromScrap, setID):\n",
    "            scraped.append(result)\n",
    "    return scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of use\n",
    "Let's do research on the data analyst post in France in the city of Rennes.  \n",
    "Preview the 2nd item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'FR',\n",
       " 'url': 'https://www.indeed.com/viewjob?jk=336e0f519279fdc3',\n",
       " 'description': \"iMSA fournit des solutions informatiques clés en main dédiées aux métiers de la sphère sociale : prestations familiales, remboursements médicaux, retraite... Ces systèmes sont utilisés au quotidien par des dizaines de milliers d'agents des caisses de MSA et des millions d'adhérents MSA (2e système de protection sociale français, 16 000 collaborateurs) et par les entreprises et institutions partenaires. Notre collectif de 800 salariés est présent sur tous les métiers de l'informatique et les technologies majeures les plus avancées (Big Data, Moteurs de règles, technologie web…).Nous recherchons pour notre site de Mordelles (35) un/une Analyste Expert Applicatif (H/F)Au sein du pôle Applications Métiers de la Direction Services, vous aurez pour principales missions de :Assurer l’expertise technique et fonctionnelle et l’assistance utilisateur sur les applicatifs déployés sur le périmètre qui vous sera confié.Réaliser l’interface entre la Maîtrise d’œuvre et les Utilisateurs, entre l’Exploitation/Production et les équipes techniques.Effectuer le suivi des traitements, analyser les résultats, étudier et mettre en place des solutions palliatives.Piloter l’installation et les montées de version de produit, Organiser et suivre les qualifications, Informer les utilisateurs et suivre le déploiement.Prendre en charge des études ponctuelles sur des dossiers spécifiques en lien avec le Responsable du Domaine.*Profil recherché : Etudes supérieures, ayant, a minima, 5 à 10 années d’expérience en informatique et une expérience d’animation ou coordination d’un collectif de travail.Capacité à dialoguer avec les utilisateurs et la Maîtrise d’oeuvre, à évoluer sur plusieurs périmètres applicatifs et à se les approprier, à répondre par son engagement au niveau de service à rendre dans un centre de services et d’exploitation.Capacité à s’intégrer, en agilité, sur les processus de Qualification, de gestion des problèmes avec la Maîtrise d’Ouvrage et la Maîtrise d’Oeuvre.Capacité d’animation de réunion, d’élaboration de synthèse, de compte-rendu.Capacité à coordonner un groupe de contributeurs.Curiosité et centres d’intérêt à la fois techniques et fonctionnels.Connaissance d’un OS de type UNIX gros système.Connaissance SQL.iMSA valorise la diversité des profils de ses collaborateurs et favorise l’équité en matière d’emploi.Type d'emploi : Temps plein, CDIExpérience:analyste expert applicatif (h/f) ou similaire: 5 ans (Requis)Formation:Bac +4 (Maîtrise) (Requis)\",\n",
       " 'header': 'Analyste Expert Applicatif (H/F)',\n",
       " 'city': 'Mordelles (35)',\n",
       " 'company': 'iMSA',\n",
       " 'type': '',\n",
       " 'category': '',\n",
       " 'posted': '+ 2019-12-22T00:30:52'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfJob = IndeedScrap([\"Data Analyst rennes\"],[\"FR\"])\n",
    "listOfJob[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
