{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MonsterScrap doc\n",
    "These functions allow to perform web scrapping on the Monster platform, to collect job detail.\n",
    "____\n",
    "Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, HTTPError\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from bs4 import BeautifulSoup\n",
    "from unicodedata import normalize\n",
    "from json import loads\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *scrapBody()* function allows to take the body part of an html document from the URL.  \n",
    "This is to avoid redundant code in the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapBody(url):\n",
    "    with urlopen(url) as response:\n",
    "        body = BeautifulSoup(response.read(), 'html.parser').body\n",
    "    return body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *idFromLink()* function allows to extract the job ID from link.\n",
    "IDs come in two different forms:\n",
    "- A series of 9 numbers\n",
    "- A string of characters xxxxxxxxxx-xxxx-xxxx-xxxx-xxxxxx-xxxxxxxxxxxx\n",
    "\n",
    "Links on Monster come in 3 different forms: \n",
    "- Pages generated with ASP including the ID, composed only by 9 digits\n",
    "- The form \\*/monster/\\* with the ID, composed by string, which is the standard form for the main site\n",
    "- \"job offer\" *(in the language of the country)* with the ID in figures at the end, which is the standard form for each sub-sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idFromLink(link):\n",
    "    if \".aspx\" in link:\n",
    "        jobID = link[-14:-5]\n",
    "    elif \"/monster/\" in link:\n",
    "        jobID = re.findall(r'monster/.+?\\?', link)[0][8:-1]\n",
    "    else:\n",
    "        jobID = link[link.rfind('/')+1:]\n",
    "    return jobID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _scrapMonsterID()_ function extracts the *jobIDs* for each job and country from the search results provided by Monster.\n",
    "The [monster.co.uk](https://www.monster.co.uk/internationalJobs) site has access to the main master database, unlike the sub-sites which only have access to their own country the database.\n",
    "\n",
    "A query allows to have the total match if it exists. If there are more than 5000, the division *resultCountLabel* displays \"5000+\". If there is none, the division does not appear on the page.\n",
    "\n",
    "During the page browsing, $p$ greater than 1, the site has a behavior that displays the absence of results on a page $p$ while there are some on the page $p+1$ and therefore ignores the 20 results of the page $p$. The function counts it as an error.\n",
    "\n",
    "The absence of *resultCountLabel* is not interpreted as the end of the results unless the size of the list of jobs covered is equal to (or greater than) the match minus the number of jobs ignored by the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapMonsterID(searchList, countryList):\n",
    "    setID = set()\n",
    "    for search in searchList:\n",
    "        search = search.replace(\" \",\"+\")\n",
    "        for country in countryList:\n",
    "            match = 5001\n",
    "            error = 0\n",
    "            listID = set()\n",
    "            page = 1\n",
    "            while True:\n",
    "                url = \"https://www.monster.co.uk/medley?q={}&fq=countryabbrev_s%3A{}&pg={}\".format(\n",
    "                    search, country, page)\n",
    "                try:\n",
    "                    body = scrapBody(url)\n",
    "                except HTTPError:\n",
    "                    break\n",
    "                else:\n",
    "                    if body.find(id=\"resultCountLabel\") is None:\n",
    "                        if len(listID) == 0:\n",
    "                            break\n",
    "                        else:\n",
    "                            error += 1\n",
    "                            if len(listID) >= (match - 20 * error):\n",
    "                                break\n",
    "                            else:\n",
    "                                page += 1\n",
    "                                continue\n",
    "                    else:\n",
    "                        match = int(\n",
    "                            re.sub(\n",
    "                                \"\\D\", \"\",\n",
    "                                body.find(\n",
    "                                    id=\"resultCountLabel\").text.split()[-1]))\n",
    "                        links = [\n",
    "                            link.a.attrs['href']\n",
    "                            for link in body.find_all(\"div\", class_=\"jobTitle\")\n",
    "                        ]\n",
    "                        listID = {idFromLink(link) for link in links}\n",
    "                        page += 1\n",
    "                setID = setID.union(listID)\n",
    "    return setID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dicoFromJson()* function normalizes the data of the request response. For a *jobID*, it collects information about the ad, the company and the specificities of the job in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicoFromJson(jobID):\n",
    "    url = \"https://job-openings.monster.com/v2/job/pure-json-view?jobid={}\".format(\n",
    "        jobID)\n",
    "    try:\n",
    "        query = urlopen(url).read()\n",
    "    except HTTPError:\n",
    "        return {}\n",
    "    dico = json.loads(\n",
    "        normalize('NFKD', query.decode('utf-8')).encode('ascii', 'ignore'))\n",
    "\n",
    "    general = ((\"description\", \"jobDescription\"),\n",
    "               (\"country\", \"jobLocationCountry\"),\n",
    "               (\"city\", \"jobLocationCity\"),\n",
    "               (\"posted\", \"postedDate\"))\n",
    "    company = ((\"header\", \"companyHeader\"),\n",
    "               (\"company\", \"name\"))\n",
    "    tracks = ((\"type\", \"eVar33\"),\n",
    "              (\"category\", \"eVar28\"))\n",
    "\n",
    "    ginfo, cinfo, tinfo = {}, {}, {}\n",
    "    for g in general:\n",
    "        try:\n",
    "            ginfo[g[0]] = normalize(\n",
    "                \"NFKD\",\n",
    "                \" \".join(BeautifulSoup(dico[g[1]], 'lxml').get_text().split()))\n",
    "        except KeyError:\n",
    "            ginfo[g[0]] = \"\"\n",
    "    for c in company:\n",
    "        try:\n",
    "            cinfo[c[0]] = BeautifulSoup(dico[\"companyInfo\"][c[1]],\n",
    "                                        'lxml').get_text().rstrip()\n",
    "        except KeyError:\n",
    "            cinfo[c[0]] = \"\"\n",
    "    for t in tracks:\n",
    "        try:\n",
    "            tinfo[t[0]] = BeautifulSoup(dico[\"adobeTrackingProperties\"][t[1]],\n",
    "                                        'lxml').get_text().rstrip()\n",
    "        except KeyError:\n",
    "            tinfo[t[0]] = \"\"\n",
    "    \n",
    "    dico = {**ginfo, **cinfo, **tinfo}\n",
    "    dico[\"url\"] = \"https://job-openings.monster.co.uk/monster/{}\".format(jobID)\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MonsterScrap()** is the main function which collects and standardizes data on the Monster site.  \n",
    "Threads are used depending on the size of the results for data normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonsterScrap(searchList, countryList):\n",
    "    scraped = list()\n",
    "    setID = scrapMonsterID(searchList, countryList)\n",
    "    if len(setID) < 20:\n",
    "        workers = len(setID)\n",
    "    else:\n",
    "        workers = len(setID) / 5\n",
    "    with ThreadPoolExecutor(workers) as executor:\n",
    "        for result in executor.map(dicoFromJson, setID):\n",
    "            scraped.append(result)\n",
    "    return scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = MonsterScrap([\"Data Scientist\"],[\"UK\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
